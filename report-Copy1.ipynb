{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are differences between logistic regression and linear regression?\n",
    "\n",
    "Linear regression works on continuous variables against to that logistic regression works\n",
    "on categorical limited variables. For example, if you are trying to the prediction on weights,\n",
    "heights you should use linear regression but if you want to make a prediction about colors\n",
    "or categorical classes you should use logistic regression. Linear regression is based on least\n",
    "square estimation it minimizes the sum of the squared distances of each observed response\n",
    "to its fitted value. While logistic regression is based on Maximum Likelihood Estimation it\n",
    "maximizes the Probability of Y given X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are differences between logistic regression and naive bayes methods?\n",
    "\n",
    "Naive Bayes and Logistic Regression are a ”generative-discriminative pair,” meaning they\n",
    "have the same model form of a linear classifier, but they estimate parameters in different\n",
    "ways. Both logistic regression and Naive Bayes have the same hypothesis space, but optimize\n",
    "different objective functions. In particular logistic regression maximizes;\n",
    "\n",
    "$$\\sum _ { i } \\log P _ { \\theta } \\left( y _ { i } | x _ { i } \\right)$$\n",
    "whereas Naive Bayes maximizies;\n",
    "$$\\sum _ { i } \\log P _ { \\theta } \\left( y _ { i } | x _ { i } \\right) + \\sum _ { i } \\log P _ { \\theta , \\phi } \\left( x _ { i } \\right)$$\n",
    "\n",
    "Naive Bayes is the better choice for small data sets (Logistic Regression will overfit). For\n",
    "large data sets, the winner is Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which of the following statements are true?\n",
    "\n",
    "$\\cdot$ A two layer (one input layer, one output layer; no hidden layer) neural network\n",
    "can represent the XOR function. $-->T$\n",
    "\n",
    "$\\cdot$ Any logical function over binary-valued $( 0$ or 1$)$ inputs $x _ { 1 }$ and $x _ { 2 }$ can be $($ ap-\n",
    "proximately $)$ represented using some neural network. $-->T$\n",
    "\n",
    "$\\cdot$ Suppose you have a multi-class classification problem with three classes,\n",
    "trained with a 3 layer network. Let $a _ { 1 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 1 }$ be the activation of\n",
    "the first output unit and similarly $a _ { 2 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 2 }$ and $a _ { 3 } ^ { ( 3 ) } = \\left( h _ { \\Theta } ( x ) \\right) _ { 3 } .$ Then\n",
    "for any input $x ,$ it must be the case that that $a _ { 1 } ^ { ( 3 ) } + a _ { 2 } ^ { ( 3 ) } + a _ { 3 } ^ { ( 3 ) } = 1$ $-->F$\n",
    "\n",
    "$\\cdot$ The activation values of the hidden units in a neural network, with the sigmoid\n",
    "activation function applied at every layer, are always in the range $( 0,1 )$ . $-->T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to decide the number of hidden layers and nodes in a hidden layer?\n",
    "\n",
    "With cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Classification of Flowers using Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   INTRODUCTIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing a system for classification of flowers is a difficult task because of considerable similarities among different classes. Applications of classification of flowers can be found useful in floriculture, flower searching for patent analysis, etc. In such cases, automation of flower classification is essential. Since these activities are done manually and are very labor intensive, automation of the classification of flower images is a necessary task.\n",
    "\n",
    "In this assignment we have a data collection of 3000 flowers belonging to five different classes. We will try to create artificial neural network by using these flowers as training data. Firstly we will try to classify by using single layer neural network, and then compare the results by trying to classify with multilayer neural network and try to determine what is the most appropriate structure for flower classification as a result of our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How artificial neural networks work\n",
    "\n",
    "A neural network usually involves a large number of processors operating in parallel and arranged in tiers. The first tier receives the raw input information analogous to optic nerves in human visual processing. Each successive tier receives the output from the tier preceding it, rather than from the raw input in the same way neurons further from the optic nerve receive signals from those closer to it. The last tier produces the output of the system. Each processing node has its own small sphere of knowledge, including what it has seen and any rules it was originally programmed with or developed for itself. \n",
    "\n",
    "Neural networks are notable for being adaptive, which means they modify themselves as they learn from initial training and subsequent runs provide more information about the world. The most basic learning model is centered on weighting the input streams, which is how each node weights the importance of input from each of its predecessors. Inputs that contribute to getting right answers are weighted higher.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How neural networks learn\n",
    "Unlike other algorithms, neural networks with their deep learning cannot be programmed directly for the task. Rather, they have the requirement, just like a child’s developing brain, that they need to learn the information. The learning strategies go by three methods:\n",
    "\n",
    "- Supervised learning: This learning strategy is the simplest, as there is a labeled dataset, which the computer goes through, and the algorithm gets modified until it can process the dataset to get the desired result.\n",
    "\n",
    "- Unsupervised learning: This strategy gets used in cases where there is no labeled dataset available to learn from. The neural network analyzes the dataset, and then a cost function then tells the neural network how far off of target it was. The neural network then adjusts to increase accuracy of the algorithm.\n",
    "\n",
    "- Reinforced learning: In this algorithm, the neural network is reinforced for positive results, and punished for a negative result, forcing the neural network to learn over time.\n",
    "\n",
    "We will use supervised learning method in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramater Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self,layersize,nodesize,activation,shape1,shape2):\n",
    "        #parameters\n",
    "        self.inputSize = shape1\n",
    "        self.outputSize = shape2\n",
    "        self.batch= 0\n",
    "        self.batcherror= np.zeros((1,shape2))\n",
    "        self.hiddenSize = nodesize\n",
    "        self.layersize =layersize\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activationfunc=self.sigmoid\n",
    "            self.activationDerivative = self.derivative_sigmoid\n",
    "        elif activation == \"relu\":\n",
    "            self.activationfunc = self.ReLU\n",
    "            self.activationDerivative = self.derivative_Relu\n",
    "\n",
    "        self.W=list()\n",
    "        self.B=list()\n",
    "\n",
    "        for i in range(self.layersize+1):\n",
    "            if i ==0:\n",
    "                if layersize !=0:\n",
    "                    self.W.append(2*np.random.random([self.inputSize, self.hiddenSize])-1)\n",
    "                    self.B.append(np.ones((1,self.hiddenSize)))\n",
    "\n",
    "                else:\n",
    "                    self.W.append(2*np.random.random([self.inputSize,self.outputSize])-1) \n",
    "                    self.B.append(np.ones((1,self.outputSize)))\n",
    "\n",
    "            elif i == layersize:\n",
    "                self.W.append(2*np.random.random([self.hiddenSize, self.outputSize])-1)\n",
    "                self.B.append(np.ones((1,self.outputSize)))\n",
    "            else:\n",
    "                self.W.append(2*np.random.random([self.hiddenSize, self.hiddenSize])-1)\n",
    "                self.B.append(np.ones((1,self.hiddenSize)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll first initialize the weight matrices and the bias vectors. It’s important to note that we shouldn’t initialize all the parameters to zero because doing so will lead the gradients to be equal and on each iteration the output would be the same and the learning algorithm won’t learn anything. Therefore, it’s important to randomly initialize the parameters to values between 0 and 1.\n",
    "\n",
    "And we need to determine which activation function to use. I used two different activation function :\n",
    "\n",
    "- \"sigmoid\" : The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output.Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "- \"ReLu\" : Range: [0 to infinity] That all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagate & Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural step to do after initialising the model at random, is to check its performance.\n",
    "We start from the input we have, we pass them through the network layer and calculate the actual output of the model streightforwardly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "raw_mimetype": "text/x-python"
   },
   "outputs": [],
   "source": [
    " def forward(self, X):\n",
    "        #forward propagation through our network\n",
    "\n",
    "        self.outsa=list()\n",
    "        self.z = X\n",
    "        self.outsa.append(X)\n",
    "        for i in range(len(self.W)):\n",
    "\n",
    "            self.z = self.z.dot(self.W[i])\n",
    "            self.z+= self.B[i]\n",
    "            self.z = self.activationfunc(self.z)\n",
    "            self.outsa.append(self.z)\n",
    "        \n",
    "        outsoft = self.softmax(self.z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is called forward-propagation, because the calculation flow is going in the natural forward direction from the input -> through the neural network -> to the output.\n",
    "\n",
    "At the \"Loss\" stage, in one hand, we have the actual output of the randomly initialize neural network. On the other hand, we have the desired output we would like the network to learn.\n",
    "\n",
    "Here we use cross-entropy as loss function. Of course, before we go through this softmax classifier, we turn our results into probability distribution. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is the Softmax\n",
    "\n",
    "Softmax function takes an N-dimensional vector of real numbers and transforms\n",
    "it into a vector of real number in range $( 0,1 )$ which add upto $ p _ { i } = \\frac { e ^ { a _ { i } } } { \\sum _ { k = 1 } ^ { N } e _ { k } ^ { a } }$\n",
    "\n",
    "As the name suggests, softmax function is a “soft” version of max function. Instead of selecting one maximum value, it breaks the whole $( 0,1 )$ with maximal element getting the largest portion of the distribution, but other smaller elements getting some of it as well.\n",
    "\n",
    "This property of softmax function that it outputs a probability distribution makes it suitable for probabilistic interpretation in classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def softmax(self,x):\n",
    "        exps = np.exp(x)\n",
    "        return exps / np.sum(exps)\n",
    "# this is simply implement for softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is the Cross-Entropy\n",
    "\n",
    "Cross entropy indicates the distance between what the model believes the output\n",
    "distribution should be, and what the original distribution really is. It is defined\n",
    "as, $H ( y , p ) =  - \\sum \\mathrm { yi } \\cdot \\log ( \\mathrm { pi } ) + ( 1 - \\mathrm { yi } ) \\cdot \\log ( 1 - \\mathrm { pi } )$ Cross entropy measure is a widely used alternative of squared error. It is used when node activations can be understood as representing the probability that each hypothesis might be true, i.e. when the output is a probability distribution. Thus it is used as a loss function in neural networks which have softmax activations in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def cross_entropy(self,out,y):\n",
    "\n",
    "        #E = – ∑ ci . log(pi) + (1 – ci ). log(1 – pi)\n",
    "        log_likelihood =-((y*np.log10(out) ) + ((1-y) * np.log10(1-out)))\n",
    "\n",
    "        return log_likelihood.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the N-gram Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram modeling is a popular feature identification and analysis approach used in\n",
    "language modeling and natural language processing fields.\n",
    "\n",
    "N-gram is a contiguous sequence of items with length n. It could be a sequence of\n",
    "words, bytes, syllables, or characters. The most used n-gram models in text categorization\n",
    "are word-based and character-based n-grams. Examples of n-gram models commonly\n",
    "used include unigram (n=1), bigram (n=2),etc.\n",
    "\n",
    "When building an n-gram based classifier, the size n is usually a fixed number\n",
    "throughout the whole corpus. The unigrams are commonly known as “the bag of words”\n",
    "model. The bag of words model does not take into consideration the order of the phrase\n",
    "in contrast to a higher order n-gram model. The n-gram model is one of the basic and\n",
    "efficient models for text categorization and language processing. It allows automatic\n",
    "capture of the most frequent words in the corpus; it can be applied to any language since\n",
    "it does not need segmentation of the text in words. Furthermore, it is flexible against\n",
    "spelling mistake and deformations since it recognizes particles of the phrase/words.\n",
    "\n",
    "In this Assignment, we will be using word-based n-gram model to represent the context\n",
    "of the document and generate features to classify the document. One of the goals of this\n",
    "assignment is to develop a simple n-gram based classifier to differentiate between fake and real\n",
    "opinions.The idea is to generate various sets of n-gram frequency profiles from the\n",
    "training data to represent fake and truthful opinions. We used two values of n to\n",
    "generate and extract the n-gram features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we used the unigram structure, we kept the words singularly in the structure of the bag of words and we tried to determine the class of the new data by calculating the $multinomal\\,naive\\,bayes$ probabilities according to the document frequencies of these words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application results are below ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classified news: 422\n",
      "Steamming : False\n",
      "TF-IDF : False\n",
      "The occurrences of words : 1\n",
      "Stopwords: None\n",
      "Accuracy:  86.29856850715747\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "main.main(1,PrintCommand=\"General Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we used the unigram structure, we kept the words pairs in the structure of the bag of words and we tried to determine the class of the new data by calculating the multinomal naive bayes probabilities according to the document frequencies of these words.\n",
    "\n",
    "While this method is applied, $token$ is added since the probability of words being the beginning of the sentence or the end of the sentence is also important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application results are below ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct classified news: 418\n",
      "Steamming : False\n",
      "TF-IDF : False\n",
      "The occurrences of words : 2\n",
      "Stopwords: None\n",
      "Accuracy:  85.48057259713701\n",
      "####################################\n"
     ]
    }
   ],
   "source": [
    "main.main(2,PrintCommand=\"General Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I applied $TF-IDF$ to normalize the word frequencies. So i could decide which words were more important for the document.Then I listed which words' presences and absences could be effective for the classification.\n",
    "\n",
    "In this section, I did the only analysis for the binary situations because the absence words in the false news that strengthened the possibility real news, likewise other situations include cross possibility.\n",
    "\n",
    "Repeating my implementation for unigram and bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.In Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose presence most strongly predicts that the news is real.\n",
      "And whose absence most strongly predicts that the news is fake.\n",
      "\n",
      "  Word/WordPairs  Frequency\n",
      "0          korea  17.346456\n",
      "1         travel  14.625705\n",
      "2       turnbull  14.284775\n",
      "3      australia  10.034324\n",
      "4        climate   7.774017\n",
      "5          paris   6.784150\n",
      "6        refugee   6.766278\n",
      "7         debate   5.928145\n",
      "8           asia   5.711946\n",
      "9          flynn   5.455626\n"
     ]
    }
   ],
   "source": [
    "main.main(1,tfidf=True,PrintCommand=\"presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose absence most strongly predicts that the news is real.\n",
      "And whose presence most strongly predicts that the news is fake.\n",
      "\n",
      "  Word/WordPairs  Frequency\n",
      "0       breaking   6.244817\n",
      "1          soros   4.452072\n",
      "2          woman   3.473386\n",
      "3          steal   3.381297\n",
      "4           duke   3.132043\n",
      "5         reason   3.057646\n",
      "6      interview   2.849209\n",
      "7             dr   2.819662\n",
      "8       homeless   2.798578\n",
      "9             my   2.732481\n"
     ]
    }
   ],
   "source": [
    "main.main(1,tfidf=True,PrintCommand=\"absence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.In Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose presence most strongly predicts that the news is real.\n",
      "And whose absence most strongly predicts that the news is fake.\n",
      "\n",
      "     Word/WordPairs  Frequency\n",
      "0       north korea  12.453395\n",
      "1        travel ban   9.930787\n",
      "2         ban _eos_   7.058293\n",
      "3       korea _eos_   6.061345\n",
      "4      _s_ turnbull   4.586553\n",
      "5      trump travel   4.498065\n",
      "6  malcolm turnbull   3.964054\n",
      "7     trumps travel   3.827668\n",
      "8       james comey   3.685592\n",
      "9    comments _eos_   3.495364\n"
     ]
    }
   ],
   "source": [
    "main.main(2,tfidf=True,PrintCommand=\"presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose absence most strongly predicts that the news is real.\n",
      "And whose presence most strongly predicts that the news is fake.\n",
      "\n",
      "   Word/WordPairs  Frequency\n",
      "0       _s_ watch   5.008082\n",
      "1     _s_ comment   4.463473\n",
      "2    _s_ breaking   4.233264\n",
      "3       trump won   2.782663\n",
      "4      daily wire   2.602999\n",
      "5      wire _eos_   2.602999\n",
      "6      voting for   2.569260\n",
      "7        will win   2.294462\n",
      "8       fame star   2.266078\n",
      "9  breaking trump   2.144844\n"
     ]
    }
   ],
   "source": [
    "main.main(2,tfidf=True,PrintCommand=\"absence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are insignificant words in a language that will create noise when used as\n",
    "features in text classification. These are words commonly used in a lot sentences to help\n",
    "connect thought or to assist in the sentence structure. Articles, prepositions and\n",
    "conjunctions and some pronouns are considered stop words. We removed common words\n",
    "such as, a, about, an, are, as, at, be, by, for, from, how, in, is, of, on, or, that, the, these,\n",
    "this, too, was, what, when, where, who, will, etc. Those words were removed from each\n",
    "document. And classification successes re-evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.In Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose presence most strongly predicts that the news is real.\n",
      "And whose absence most strongly predicts that the news is fake.\n",
      "\n",
      "  Word/WordPairs  Frequency\n",
      "0          korea  19.479725\n",
      "1       turnbull  15.795719\n",
      "2         travel  15.778778\n",
      "3      australia  11.153182\n",
      "4        climate   8.179058\n",
      "5        refugee   7.159126\n",
      "6          paris   7.103846\n",
      "7         debate   6.317261\n",
      "8           asia   6.135109\n",
      "9       congress   5.973006\n"
     ]
    }
   ],
   "source": [
    "main.main(1,stopWords=\"english\",tfidf=True,PrintCommand=\"presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose absence most strongly predicts that the news is real.\n",
      "And whose presence most strongly predicts that the news is fake.\n",
      "\n",
      "  Word/WordPairs  Frequency\n",
      "0       breaking   6.750180\n",
      "1          soros   4.805717\n",
      "2          steal   3.967294\n",
      "3          woman   3.854602\n",
      "4         reason   3.508850\n",
      "5           duke   3.389549\n",
      "6      interview   3.158593\n",
      "7             dr   3.100562\n",
      "8       homeless   2.946114\n",
      "9      landslide   2.903458\n"
     ]
    }
   ],
   "source": [
    "main.main(1,stopWords=\"english\",tfidf=True,PrintCommand=\"absence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.In Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose presence most strongly predicts that the news is real.\n",
      "And whose absence most strongly predicts that the news is fake.\n",
      "\n",
      "     Word/WordPairs  Frequency\n",
      "0       north korea  14.366889\n",
      "1        travel ban  10.909364\n",
      "2         ban _eos_   7.634519\n",
      "3       korea _eos_   7.003623\n",
      "4      _s_ turnbull   5.723430\n",
      "5      trump travel   4.941856\n",
      "6  malcolm turnbull   4.455249\n",
      "7     trumps travel   4.272293\n",
      "8    comments _eos_   4.210005\n",
      "9   australia _eos_   4.059922\n"
     ]
    }
   ],
   "source": [
    "main.main(2,stopWords=\"english\",tfidf=True,PrintCommand=\"presence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 words whose absence most strongly predicts that the news is real.\n",
      "And whose presence most strongly predicts that the news is fake.\n",
      "\n",
      "   Word/WordPairs  Frequency\n",
      "0       _s_ watch   6.038749\n",
      "1     _s_ comment   5.572620\n",
      "2    _s_ breaking   4.798081\n",
      "3       trump won   4.240658\n",
      "4      daily wire   2.875408\n",
      "5      wire _eos_   2.875408\n",
      "6       fame star   2.572662\n",
      "7  breaking trump   2.410352\n",
      "8    george soros   2.364295\n",
      "9         _s_ cnn   2.272511\n"
     ]
    }
   ],
   "source": [
    "main.main(2,stopWords=\"english\",tfidf=True,PrintCommand=\"absence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords Removal Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally stopwords are words that are likely to go through a large number of all classes.Therefore it is illogical to include it in the words to be considered for classification.But this situation may vary according to the classified content.For example, when classifying using unigram, it may increase the success of classification. But some words can make different attributes with stopwords while using bigram and this stuation may decrease classification success. \n",
    "\n",
    "For the classification we made, it may vary according to the given training data. For example;\n",
    "        \n",
    "        The word \"to\" may be used in real news, but may never be used in fake news.\n",
    "        In this case the new news may contain a large frequency of words \"to\" and this may raise the probability of real news classification. But when we remove this word, the probability of being sent to the fake class may increase.\n",
    "        As another example of \"to trump\" may increase the probability of fake class. When we remove the word \"to\", the probability can be greatly reduced. This is also a large number of situations when using the bigram.\n",
    "        \n",
    "When this type of situation is taken into consideration, the result is negative for the training data we use. so I think it's not logical to use stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tokenizing the data, the next step is to transform the tokens into a standard form.\n",
    "Stemming, simply, is changing the words into their original form, and decreasing the\n",
    "number of word types or classes in the data. For example, the words “Running,” ”Ran”\n",
    "and “Runner” will be reduced to the word “run.” We use stemming to make classification\n",
    "faster and efficient.\n",
    "\n",
    "This may affect the success rate according to the given data as in stopwords. For this classification, I can say that this is not the right method.Only increases when bigram is used, but the same success cannot be achieved when using unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I make classification for the given test data, the results obtained for all cases are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    N_gram Stop words   Stem  TF-IDF  Correct classified   Accuracy\n",
      "0        1       None  False   False                 422  86.298569\n",
      "1        1       None  False    True                 420  85.889571\n",
      "2        1       None   True   False                 414  84.662577\n",
      "3        1       None   True    True                 409  83.640082\n",
      "4        1    english  False   False                 412  84.253579\n",
      "5        1    english  False    True                 409  83.640082\n",
      "6        1    english   True   False                 396  80.981595\n",
      "7        1    english   True    True                 401  82.004090\n",
      "8        2       None  False   False                 418  85.480573\n",
      "9        2       None  False    True                 417  85.276074\n",
      "10       2       None   True   False                 421  86.094070\n",
      "11       2       None   True    True                 414  84.662577\n",
      "12       2    english  False   False                 396  80.981595\n",
      "13       2    english  False    True                 395  80.777096\n",
      "14       2    english   True   False                 401  82.004090\n",
      "15       2    english   True    True                 396  80.981595\n"
     ]
    }
   ],
   "source": [
    "main.All_Results(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown by the results, it is seen that there are two most suitable methods according to the given data after applying all the effects. Using unigram while using the data in a lean form is one of them. The other one is to use the bigram with stemming words. In other cases the classification success was adversely affected.\n",
    "\n",
    "As a result of our operations, by using naive bayes when making classification based on words, by applying various operations on words, we have taught that different results can be obtained when we shape similarity situations according to different criteria, different meanings can be obtained by looking at the relationship of words with each other.According to this information learned in some cases when the classification is done better in some cases has been determined that worse classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                          $$\\\\Muhammed\\,Enes\\\\KOÇAK\\\\21427119$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
